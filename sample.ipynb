{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from einops import rearrange\n",
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow import Raft_Large_Weights\n",
    "from kinetics import Kinetics700InterpolateBase\n",
    "from annotator.hed import TorchHEDdetector\n",
    "\n",
    "\n",
    "p = \"./train_log/kin_hed_dropout1/lightning_logs/version_1/checkpoints/epoch=6-step=595385.ckpt\"\n",
    "# p = \"./models/inter1.ckpt\"\n",
    "# p = \"./train_log/kin_hed_2/lightning_logs/version_0/checkpoints/epoch=1-step=119930.ckpt\"\n",
    "# p = './train_log/kin_hed_cross_3/lightning_logs/version_0/checkpoints/epoch=4-step=412223.ckpt'\n",
    "\n",
    "model = create_model(\"./models/cldm_v15_cross.yaml\").cuda()\n",
    "\n",
    "model.load_state_dict(load_state_dict(p, location=\"cuda\"))\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "frozenClipImageEmbedder = model.style_encoder\n",
    "\n",
    "\n",
    "def disabled_train(self, mode=True):\n",
    "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
    "    does not change anymore.\"\"\"\n",
    "    return self\n",
    "\n",
    "\n",
    "flownet = raft_large(weights=Raft_Large_Weights.DEFAULT, progress=False).to(\"cuda\")\n",
    "flownet = flownet.eval()\n",
    "flownet.train = disabled_train\n",
    "\n",
    "\n",
    "def warp_frame(start_frame, flow, round=True):\n",
    "    warped_frame = torch.ones_like(start_frame) * -1.0\n",
    "    target_mask = torch.ones_like(start_frame)[:, 0] * -1.0\n",
    "    target_mask = rearrange(target_mask, \"b h w -> b 1 h w\")\n",
    "    flowlength = torch.sqrt(torch.sum(flow**2, dim=1))  # length of the flow vector\n",
    "    flowlength = rearrange(flowlength, \"n h w -> n (h w)\")\n",
    "    source_indices = torch.argsort(\n",
    "        flowlength, dim=-1\n",
    "    )  # sort by  warping pixels with small flow first\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.arange(start_frame.size(2)),\n",
    "        torch.arange(start_frame.size(3)),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    grid = (\n",
    "        torch.cat([xx.unsqueeze(0), yy.unsqueeze(0)], dim=0)\n",
    "        .unsqueeze(0)\n",
    "        .repeat(start_frame.size(0), 1, 1, 1)\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    if round:\n",
    "        vgrid = grid + torch.round(flow).long()\n",
    "    else:\n",
    "        vgrid = grid + flow.long()\n",
    "\n",
    "    maskw = torch.logical_and(\n",
    "        vgrid[:, 0, :, :] >= 0, vgrid[:, 0, :, :] < start_frame.size(3)\n",
    "    )\n",
    "    maskh = torch.logical_and(\n",
    "        vgrid[:, 1, :, :] >= 0, vgrid[:, 1, :, :] < start_frame.size(2)\n",
    "    )\n",
    "    mask = torch.logical_and(\n",
    "        maskw, maskh\n",
    "    )  # mask of pixels we are allowed to move to prevent out of domain mapping\n",
    "    mask = rearrange(mask, \"n h w -> n (h w)\")\n",
    "\n",
    "    for b in range(start_frame.size(0)):\n",
    "        # filter indices\n",
    "        filtered_source_indices = torch.masked_select(\n",
    "            source_indices[b], mask[b, source_indices[b]]\n",
    "        )  # only select source indices which don't map out of domain\n",
    "        source_pixels = torch.index_select(\n",
    "            rearrange(start_frame[b], \"c h w -> c (h w)\"), -1, filtered_source_indices\n",
    "        )  # order pixels from source image\n",
    "        target_indices = torch.index_select(\n",
    "            rearrange(vgrid[b], \"c h w -> c (h w)\"), -1, filtered_source_indices\n",
    "        )\n",
    "        target_indices = (\n",
    "            target_indices[1] * start_frame.size(3) + target_indices[0]\n",
    "        )  # convert to flattened indices\n",
    "\n",
    "        # create mask here for inpainting\n",
    "        temp_mask = torch.ones((start_frame.size(2) * start_frame.size(3))).cuda() * 1.0\n",
    "        temp_mask[\n",
    "            target_indices\n",
    "        ] = -1.0  # black pixels are kept, only white pixels should be masked regions\n",
    "        target_mask[b] = rearrange(temp_mask, \"(h w) -> 1 h w\", h=start_frame.size(2))\n",
    "\n",
    "        # set pixel at target_indices location\n",
    "        temp = (\n",
    "            torch.ones(\n",
    "                (start_frame.size(1), start_frame.size(2) * start_frame.size(3))\n",
    "            ).cuda()\n",
    "            * -1.0\n",
    "        )\n",
    "        temp[:, target_indices] = source_pixels\n",
    "        warped_frame[b] = rearrange(temp, \"c (h w) -> c h w\", h=start_frame.size(2))\n",
    "\n",
    "    return warped_frame, target_mask  # warped_frame is (n c h w)\n",
    "\n",
    "\n",
    "def show(w):\n",
    "    w = rearrange(w, \"b c h w -> h (b w) c\")\n",
    "    w = w.cpu().numpy()\n",
    "    w = (w + 1) * 127.5\n",
    "    w = w.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparation of Kinetics700InterpolateBase, which consists of 31157 videos representing 700 different actions.\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 40/40 [00:15<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 40/40 [00:15<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  62%|██████▎   | 25/40 [00:10<00:06,  2.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/export/home/nstracke/dev/ControlNet/sample.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# only need to potentially change this for guess mode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m model\u001b[39m.\u001b[39mcontrol_scales \u001b[39m=\u001b[39m [strength] \u001b[39m*\u001b[39m \u001b[39m13\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m samples, intermediates \u001b[39m=\u001b[39m ddim_sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     ddim_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     B,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     shape,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     cond,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     eta\u001b[39m=\u001b[39;49meta,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     unconditional_guidance_scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     unconditional_conditioning\u001b[39m=\u001b[39;49mun_cond,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m x_samples \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode_first_stage(samples) \u001b[39m*\u001b[39m \u001b[39m127.5\u001b[39m \u001b[39m+\u001b[39m \u001b[39m127.5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhfdhgpu1.iwr.uni-heidelberg.de/export/home/nstracke/dev/ControlNet/sample.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m x_samples \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39mrearrange(x_samples, \u001b[39m\"\u001b[39m\u001b[39mb c h w -> b h w c\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/models/diffusion/ddim.py:103\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold, ucg_schedule, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m size \u001b[39m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m    101\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData shape for DDIM sampling is \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, eta \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m samples, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_sampling(conditioning, size,\n\u001b[1;32m    104\u001b[0m                                             callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    105\u001b[0m                                             img_callback\u001b[39m=\u001b[39;49mimg_callback,\n\u001b[1;32m    106\u001b[0m                                             quantize_denoised\u001b[39m=\u001b[39;49mquantize_x0,\n\u001b[1;32m    107\u001b[0m                                             mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0,\n\u001b[1;32m    108\u001b[0m                                             ddim_use_original_steps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    109\u001b[0m                                             noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[1;32m    110\u001b[0m                                             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    111\u001b[0m                                             score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[1;32m    112\u001b[0m                                             corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[1;32m    113\u001b[0m                                             x_T\u001b[39m=\u001b[39;49mx_T,\n\u001b[1;32m    114\u001b[0m                                             log_every_t\u001b[39m=\u001b[39;49mlog_every_t,\n\u001b[1;32m    115\u001b[0m                                             unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    116\u001b[0m                                             unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[1;32m    117\u001b[0m                                             dynamic_threshold\u001b[39m=\u001b[39;49mdynamic_threshold,\n\u001b[1;32m    118\u001b[0m                                             ucg_schedule\u001b[39m=\u001b[39;49mucg_schedule\n\u001b[1;32m    119\u001b[0m                                             )\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/models/diffusion/ddim.py:163\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold, ucg_schedule)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(ucg_schedule) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(time_range)\n\u001b[1;32m    161\u001b[0m     unconditional_guidance_scale \u001b[39m=\u001b[39m ucg_schedule[i]\n\u001b[0;32m--> 163\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_ddim(img, cond, ts, index\u001b[39m=\u001b[39;49mindex, use_original_steps\u001b[39m=\u001b[39;49mddim_use_original_steps,\n\u001b[1;32m    164\u001b[0m                           quantize_denoised\u001b[39m=\u001b[39;49mquantize_denoised, temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    165\u001b[0m                           noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout, score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[1;32m    166\u001b[0m                           corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[1;32m    167\u001b[0m                           unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    168\u001b[0m                           unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[1;32m    169\u001b[0m                           dynamic_threshold\u001b[39m=\u001b[39;49mdynamic_threshold)\n\u001b[1;32m    170\u001b[0m img, pred_x0 \u001b[39m=\u001b[39m outs\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/models/diffusion/ddim.py:211\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         c_in \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([unconditional_conditioning, c])\n\u001b[0;32m--> 211\u001b[0m     model_uncond, model_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mapply_model(x_in, t_in, c_in)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m)\n\u001b[1;32m    212\u001b[0m     model_output \u001b[39m=\u001b[39m model_uncond \u001b[39m+\u001b[39m unconditional_guidance_scale \u001b[39m*\u001b[39m (model_t \u001b[39m-\u001b[39m model_uncond)\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameterization \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/cldm/cldm.py:722\u001b[0m, in \u001b[0;36mCrossControlLDM.apply_model\u001b[0;34m(self, x_noisy, t, cond, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m     control \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_model(\n\u001b[1;32m    715\u001b[0m         x\u001b[39m=\u001b[39mx_noisy,\n\u001b[1;32m    716\u001b[0m         hint\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat(cond[\u001b[39m\"\u001b[39m\u001b[39mc_concat\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m         style_embed\u001b[39m=\u001b[39mc_embed,\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m     control \u001b[39m=\u001b[39m [c \u001b[39m*\u001b[39m scale \u001b[39mfor\u001b[39;00m c, scale \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(control, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_scales)]\n\u001b[0;32m--> 722\u001b[0m     eps \u001b[39m=\u001b[39m diffusion_model(\n\u001b[1;32m    723\u001b[0m         x\u001b[39m=\u001b[39;49mx_noisy,\n\u001b[1;32m    724\u001b[0m         timesteps\u001b[39m=\u001b[39;49mt,\n\u001b[1;32m    725\u001b[0m         context\u001b[39m=\u001b[39;49mcond_txt,\n\u001b[1;32m    726\u001b[0m         control\u001b[39m=\u001b[39;49mcontrol,\n\u001b[1;32m    727\u001b[0m         only_mid_control\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monly_mid_control,\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m eps\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/cldm/cldm.py:66\u001b[0m, in \u001b[0;36mControlledUnetModel.forward\u001b[0;34m(self, x, timesteps, context, control, only_mid_control, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h, hs\u001b[39m.\u001b[39mpop() \u001b[39m+\u001b[39m control\u001b[39m.\u001b[39mpop()], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     h \u001b[39m=\u001b[39m module(h, emb, context)\n\u001b[1;32m     68\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(h)\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/modules/diffusionmodules/openaimodel.py:84\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context)\u001b[0m\n\u001b[1;32m     82\u001b[0m     x \u001b[39m=\u001b[39m layer(x, emb)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[39m=\u001b[39m layer(x, context)\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/modules/attention.py:334\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    332\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_in(x)\n\u001b[1;32m    333\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks):\n\u001b[0;32m--> 334\u001b[0m     x \u001b[39m=\u001b[39m block(x, context\u001b[39m=\u001b[39;49mcontext[i])\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_linear:\n\u001b[1;32m    336\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/modules/attention.py:269\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward, (x, context), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint)\n",
      "File \u001b[0;32m/export/compvis-nfs/user/nstracke/dev/ControlNet/ldm/modules/diffusionmodules/util.py:113\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mEvaluate a function without caching intermediate activations, allowing for\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mreduced memory at the expense of extra compute in the backward pass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m:param flag: if False, disable gradient checkpointing.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m flag:\n\u001b[0;32m--> 113\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39;49m(params)\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39mapply(func, \u001b[39mlen\u001b[39m(inputs), \u001b[39m*\u001b[39margs)\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1642\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparameters\u001b[39m(\u001b[39mself\u001b[39m, recurse: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   1622\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \n\u001b[1;32m   1624\u001b[0m \u001b[39m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \n\u001b[1;32m   1641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1642\u001b[0m     \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters(recurse\u001b[39m=\u001b[39mrecurse):\n\u001b[1;32m   1643\u001b[0m         \u001b[39myield\u001b[39;00m param\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1668\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[39mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1663\u001b[0m \n\u001b[1;32m   1664\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[1;32m   1666\u001b[0m     \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39m_parameters\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1667\u001b[0m     prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse)\n\u001b[0;32m-> 1668\u001b[0m \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m gen:\n\u001b[1;32m   1669\u001b[0m     \u001b[39myield\u001b[39;00m elem\n",
      "File \u001b[0;32m~/miniconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1617\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m v \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m   1616\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 1617\u001b[0m memo\u001b[39m.\u001b[39;49madd(v)\n\u001b[1;32m   1618\u001b[0m name \u001b[39m=\u001b[39m module_prefix \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m module_prefix \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m k\n\u001b[1;32m   1619\u001b[0m \u001b[39myield\u001b[39;00m name, v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddim_steps = 40\n",
    "strength = 1\n",
    "eta = 0\n",
    "scale = 7\n",
    "\n",
    "dataset = Kinetics700InterpolateBase(\n",
    "    sequence_time=None,\n",
    "    sequence_length=seq_length,\n",
    "    size=512,\n",
    "    resize_size=None,\n",
    "    random_crop=None,\n",
    "    pixel_range=2,\n",
    "    interpolation=\"bicubic\",\n",
    "    mode=\"val\",\n",
    "    data_path=\"/export/compvis-nfs/group/datasets/kinetics-dataset/k700-2020\",\n",
    "    dataset_size=1.0,\n",
    "    filter_file=\"/export/home/koktay/flow_diffusion/scripts/timestamps_validation.json\",\n",
    "    flow_only=False,\n",
    "    include_full_sequence=False,\n",
    "    include_hed=True,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "dl = DataLoader(dataset, shuffle=True, batch_size=4)\n",
    "_iter = iter(dl)\n",
    "\n",
    "for i in range(250):\n",
    "    # styles, structures = get_sequence(dl)\n",
    "    batch = next(_iter)\n",
    "\n",
    "    # first element of batch (full sequence)\n",
    "    styles = batch[\"start_frame\"]\n",
    "\n",
    "    styles = (styles + 1.0) * 127.5\n",
    "    styles = styles.clip(0, 255).type(torch.uint8)\n",
    "    styles = einops.rearrange(styles, \"b h w c -> b c h w\").cuda()\n",
    "\n",
    "    control = batch[\"hed_intermediate_frame\"]\n",
    "    control = einops.rearrange(control, \"b h w c -> b c h w\").cuda()\n",
    "\n",
    "    B, C, H, W = control.shape\n",
    "\n",
    "    style_embedding = frozenClipImageEmbedder(styles)\n",
    "\n",
    "    c_style = style_embedding.last_hidden_state\n",
    "    c_embed = style_embedding.pooler_output\n",
    "    c_prompt = model.get_learned_conditioning(\n",
    "        [\"a professional, detailed, high-quality image\"] * B\n",
    "    )\n",
    "\n",
    "    uc_style = torch.zeros_like(c_style)\n",
    "    uc_embed = torch.zeros_like(c_embed)\n",
    "    uc_prompt = model.get_learned_conditioning([\"\"] * B)\n",
    "\n",
    "    cond = {\n",
    "        \"c_concat\": [control],\n",
    "        \"c_crossattn\": [c_prompt],\n",
    "        \"c_style\": [c_style],\n",
    "        \"c_embed\": [c_embed],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": [control],\n",
    "        \"c_crossattn\": [uc_prompt],\n",
    "        \"c_style\": [uc_style],\n",
    "        \"c_embed\": [uc_embed],\n",
    "    }\n",
    "    shape = (4, H // 8, W // 8)\n",
    "\n",
    "    # only need to potentially change this for guess mode\n",
    "    model.control_scales = [strength] * 13\n",
    "\n",
    "    samples, intermediates = ddim_sampler.sample(\n",
    "        ddim_steps,\n",
    "        B,\n",
    "        shape,\n",
    "        cond,\n",
    "        verbose=False,\n",
    "        eta=eta,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )\n",
    "\n",
    "    x_samples = model.decode_first_stage(samples) * 127.5 + 127.5\n",
    "\n",
    "    x_samples = einops.rearrange(x_samples, \"b c h w -> b h w c\")\n",
    "    x_samples = x_samples.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    styles = einops.rearrange(styles, \"b c h w -> b h w c\")\n",
    "    styles = styles.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    for j in range(B):\n",
    "        Image.fromarray(x_samples[j]).save(f\"samples_cross/x/img_{i}-{j}.png\")\n",
    "        Image.fromarray(styles[j]).save(f\"samples_cross/s/img_{i}-{j}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_style.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
