{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_dataset_cross import MyDataset\n",
    "import cv2\n",
    "from share import *\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "\n",
    "seed = 42 \n",
    "if False:\n",
    "    seed = random.randint(0, 65535)\n",
    "seed_everything(seed)\n",
    "\n",
    "\n",
    "dataset = MyDataset(\"kin_hed2\")\n",
    "dataloader = DataLoader(dataset, num_workers=1, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "def get_key(name: str):\n",
    "    name = name.split(\".\")[1]\n",
    "    name = name.split(\"_\")[-1]\n",
    "    name = name[3:]\n",
    "    return int(name)\n",
    "\n",
    "\n",
    "def get_sequence():\n",
    "    for batch in dataloader:\n",
    "        meta_batch = batch[\"meta\"][\"file_name\"]\n",
    "        meta = meta_batch[0]\n",
    "        meta = meta.split(\"/\")[1]\n",
    "        meta = meta.split(\"_\")[:-1]\n",
    "        meta = \"_\".join(meta)\n",
    "        print(meta)\n",
    "        styles = glob.glob(\"./data/kin_hed2/jpg/\" + meta + \"*\")\n",
    "        styles.sort(key=get_key)\n",
    "        structures = glob.glob(\"./data/kin_hed2/hint/\" + meta + \"*\")\n",
    "        structures.sort(key=get_key)\n",
    "        print(styles)\n",
    "        print(structures)\n",
    "        return styles, structures\n",
    "\n",
    "\n",
    "def get_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # img = img.astype(np.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import numpy as np\n",
    "from ldm.modules.encoders.modules import FrozenClipImageEmbedder\n",
    "import torch.nn.functional as F\n",
    "import config\n",
    "import gradio as gr\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from annotator.util import resize_image, HWC3\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/distributed/_sharded_tensor/__init__.py:8: DeprecationWarning: torch.distributed._sharded_tensor will be deprecated, use torch.distributed._shard.sharded_tensor instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/cldm_v15_cross.yaml]\n",
      "Loaded state_dict from [./train_log/kin_hed_dropout1/lightning_logs/version_1/checkpoints/epoch=6-step=595385.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "p = \"./train_log/kin_hed_dropout1/lightning_logs/version_1/checkpoints/epoch=6-step=595385.ckpt\"\n",
    "# p = \"./train_log/kin_hed_2/lightning_logs/version_0/checkpoints/epoch=1-step=119930.ckpt\"\n",
    "# p = './train_log/kin_hed_cross_3/lightning_logs/version_0/checkpoints/epoch=4-step=412223.ckpt'\n",
    "\n",
    "model = create_model(\"./models/cldm_v15_cross.yaml\").cpu()\n",
    "model.load_state_dict(load_state_dict(p, location=\"cuda\"))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "frozenClipImageEmbedder = model.style_encoder\n",
    "\n",
    "if config.save_memory:\n",
    "    model.low_vram_shift(is_diffusing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vWd1q37V6eY_000054_000064\n",
      "['./data/kin_hed2/jpg/vWd1q37V6eY_000054_000064_idx0.png', './data/kin_hed2/jpg/vWd1q37V6eY_000054_000064_idx500.png', './data/kin_hed2/jpg/vWd1q37V6eY_000054_000064_idx1000.png', './data/kin_hed2/jpg/vWd1q37V6eY_000054_000064_idx1500.png', './data/kin_hed2/jpg/vWd1q37V6eY_000054_000064_idx2000.png']\n",
      "['./data/kin_hed2/hint/vWd1q37V6eY_000054_000064_idx500.png', './data/kin_hed2/hint/vWd1q37V6eY_000054_000064_idx1000.png', './data/kin_hed2/hint/vWd1q37V6eY_000054_000064_idx1500.png', './data/kin_hed2/hint/vWd1q37V6eY_000054_000064_idx2000.png']\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  88%|████████▊ | 44/50 [00:18<00:02,  2.40it/s]"
     ]
    }
   ],
   "source": [
    "ddim_steps = 50\n",
    "strength = 1\n",
    "eta = 0\n",
    "scale = 3\n",
    "\n",
    "\n",
    "grid = []\n",
    "for i in range(5):\n",
    "    styles, structures = get_sequence()\n",
    "    styles = np.array([get_img(style) for style in styles])\n",
    "    styles = styles.copy().astype(np.float32)\n",
    "\n",
    "    structures = np.array([get_img(structure) for structure in structures])\n",
    "    structures = structures.astype(np.float32) / 255.0\n",
    "\n",
    "    B, H, W, C = structures.shape\n",
    "\n",
    "    # we only want to take the space information from the first image\n",
    "    styles_first = np.repeat(styles[[0]], B, axis=0)\n",
    "\n",
    "    control = torch.from_numpy(structures.copy()).float().cuda()\n",
    "    control = einops.rearrange(control, \"b h w c -> b c h w\")\n",
    "\n",
    "    style_embedding = frozenClipImageEmbedder(styles_first)\n",
    "\n",
    "    c_style = style_embedding.last_hidden_state\n",
    "    c_embed = style_embedding.pooler_output\n",
    "    c_prompt = model.get_learned_conditioning(\n",
    "        [\"a professional, detailed, high-quality image\"] * B\n",
    "    )\n",
    "\n",
    "    uc_style = torch.zeros_like(c_style)\n",
    "    uc_embed = torch.zeros_like(c_embed)\n",
    "    uc_prompt = model.get_learned_conditioning(\n",
    "        [\"a professional, detailed, high-quality image\"] * B\n",
    "    )\n",
    "\n",
    "    cond = {\n",
    "        \"c_concat\": [control],\n",
    "        \"c_crossattn\": [c_prompt],\n",
    "        \"c_style\": [c_style],\n",
    "        \"c_embed\": [c_embed],\n",
    "    }\n",
    "    un_cond = {\n",
    "        \"c_concat\": [control],\n",
    "        \"c_crossattn\": [uc_prompt],\n",
    "        \"c_style\": [uc_style],\n",
    "        \"c_embed\": [uc_embed],\n",
    "    }\n",
    "    shape = (4, H // 8, W // 8)\n",
    "\n",
    "    model.control_scales = [\n",
    "        strength\n",
    "    ] * 13  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "    samples, intermediates = ddim_sampler.sample(\n",
    "        ddim_steps,\n",
    "        B,\n",
    "        shape,\n",
    "        cond,\n",
    "        verbose=False,\n",
    "        eta=eta,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        unconditional_conditioning=un_cond,\n",
    "    )\n",
    "\n",
    "    d_samples = model.decode_first_stage(samples)\n",
    "    x_samples = einops.rearrange(d_samples, \"b c h w -> h (b w) c\") * 127.5 + 127.5\n",
    "    x_samples = x_samples.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "    x_samples = np.concatenate(\n",
    "        [np.zeros((512, 512, 3), dtype=np.uint8), x_samples], axis=1\n",
    "    )\n",
    "\n",
    "    f_styles = (\n",
    "        einops.rearrange(styles, \"b h w c -> h (b w) c\").clip(0, 255).astype(np.uint8)\n",
    "    )\n",
    "    final = np.concatenate([f_styles, x_samples], axis=0)\n",
    "\n",
    "    grid.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_grid = np.concatenate(grid, axis=0)\n",
    "\n",
    "Image.fromarray(f_grid).save(\"grid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 2560, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sanity_val_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
