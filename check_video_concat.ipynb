{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_dataset_concat import MyDataset\n",
    "import cv2\n",
    "from share import *\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "\n",
    "dataset = MyDataset(\"kin_hed2\")\n",
    "dataloader = DataLoader(dataset, num_workers=1, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "def get_key(name: str):\n",
    "    name = name.split(\".\")[1]\n",
    "    name = name.split(\"_\")[-1]\n",
    "    name = name[3:]\n",
    "    return int(name)\n",
    "\n",
    "\n",
    "# only returns paths\n",
    "# no actual images\n",
    "def get_sequence():\n",
    "    for batch in dataloader:\n",
    "        meta_batch = batch[\"meta\"][\"file_name\"]\n",
    "        meta = meta_batch[0]\n",
    "        meta = meta.split(\"/\")[1]\n",
    "        meta = meta.split(\"_\")[:-1]\n",
    "        meta = \"_\".join(meta)\n",
    "        print(meta)\n",
    "        styles = glob.glob(\"./data/kin_hed2/jpg/\" + meta + \"*\")\n",
    "        styles.sort(key=get_key)\n",
    "        structures = glob.glob(\"./data/kin_hed2/hint/\" + meta + \"*\")\n",
    "        structures.sort(key=get_key)\n",
    "        print(styles)\n",
    "        print(structures)\n",
    "        return styles, structures\n",
    "\n",
    "\n",
    "def get_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # img = img.astype(np.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "import numpy as np\n",
    "from ldm.modules.encoders.modules import FrozenClipImageEmbedder\n",
    "import torch.nn.functional as F\n",
    "import config\n",
    "import gradio as gr\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/nstracke/miniconda3/envs/control/lib/python3.8/site-packages/torch/distributed/_sharded_tensor/__init__.py:8: DeprecationWarning: torch.distributed._sharded_tensor will be deprecated, use torch.distributed._shard.sharded_tensor instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [./models/cldm_v15_concat.yaml]\n",
      "Loaded state_dict from [./train_log/kin_hed_concat3/lightning_logs/version_1/checkpoints/epoch=1-step=165530.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "p = \"./train_log/kin_hed_concat3/lightning_logs/version_1/checkpoints/epoch=1-step=165530.ckpt\"\n",
    "# p = './train_log/kin_hed_cross_3/lightning_logs/version_0/checkpoints/epoch=4-step=412223.ckpt'\n",
    "\n",
    "model = create_model(\"./models/cldm_v15_concat.yaml\").cpu()\n",
    "model.load_state_dict(load_state_dict(p, location=\"cuda\"))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "frozenClipImageEmbedder = FrozenClipImageEmbedder()\n",
    "frozenClipImageEmbedder = frozenClipImageEmbedder.cuda()\n",
    "\n",
    "seed = 42\n",
    "if seed == -1:\n",
    "    seed = random.randint(0, 65535)\n",
    "seed_everything(seed)\n",
    "\n",
    "if config.save_memory:\n",
    "    model.low_vram_shift(is_diffusing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k0Zrqpv7V74_000073_000083\n",
      "['./data/kin_hed2/jpg/k0Zrqpv7V74_000073_000083_idx0.png', './data/kin_hed2/jpg/k0Zrqpv7V74_000073_000083_idx500.png', './data/kin_hed2/jpg/k0Zrqpv7V74_000073_000083_idx1000.png']\n",
      "['./data/kin_hed2/hint/k0Zrqpv7V74_000073_000083_idx500.png', './data/kin_hed2/hint/k0Zrqpv7V74_000073_000083_idx1000.png']\n",
      "Data shape for DDIM sampling is (2, 4, 64, 64), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:02<00:00,  8.33it/s]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1\n",
    "ddim_steps = 20\n",
    "strength = 1\n",
    "eta = 0\n",
    "scale = 1\n",
    "\n",
    "styles, structures = get_sequence()\n",
    "styles = np.array([get_img(style) for style in styles])\n",
    "styles = styles.copy().astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "structures = np.array([get_img(structure) for structure in structures])\n",
    "structures = structures.astype(np.float32) / 255.0\n",
    "\n",
    "hints = [ np.concatenate([structure, style], axis=-1) for (structure, style) in zip(structures, styles[:-1]) ]\n",
    "hints = np.array(hints)\n",
    "\n",
    "B, H, W, C = hints.shape\n",
    "\n",
    "control = torch.from_numpy(hints.copy()).float().cuda()\n",
    "control = einops.rearrange(control, \"b h w c -> b c h w\")\n",
    "\n",
    "cond = {\n",
    "    \"c_concat\": [control],\n",
    "    \"c_crossattn\": [\n",
    "        model.get_learned_conditioning(\n",
    "            [\"\"] * B\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "un_cond = {\n",
    "    \"c_concat\": [control],\n",
    "    \"c_crossattn\": [model.get_learned_conditioning([\"\"] * B)],\n",
    "}\n",
    "shape = (4, H // 8, W // 8)\n",
    "\n",
    "model.control_scales = [\n",
    "    strength\n",
    "] * 13  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "samples, intermediates = ddim_sampler.sample(\n",
    "    ddim_steps,\n",
    "    B,\n",
    "    shape,\n",
    "    cond,\n",
    "    verbose=False,\n",
    "    eta=eta,\n",
    "    unconditional_guidance_scale=scale,\n",
    "    unconditional_conditioning=un_cond,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 64, 64])\n",
      "(1030, 516, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "d_samples = model.decode_first_stage(samples)\n",
    "x_samples = einops.rearrange(d_samples, \"b c h w -> b h w c\") * 127.5 + 127.5\n",
    "\n",
    "grid = torchvision.utils.make_grid(d_samples, nrow=1)\n",
    "grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "grid = grid * 127.5 + 127.5\n",
    "grid = grid.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "x_samples = x_samples.cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "print(grid.shape)\n",
    "print(x_samples[0].shape)\n",
    "Image.fromarray(x_samples[0]).save(\"test_img1.png\")\n",
    "Image.fromarray(grid).save(\"test_grid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 512, 512, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
